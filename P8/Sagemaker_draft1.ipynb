{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries loading\n",
    "import os\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "\n",
    "# Importing AWS libraries: S3, Sagemaker, PySpark\n",
    "# S3\n",
    "import boto3\n",
    "import botocore.session\n",
    "\n",
    "# Sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import sagemaker_pyspark\n",
    "\n",
    "# Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Dimension reduction - PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Tensorflow\n",
    "#!pip install tensorflow\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Misc\n",
    "import time\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session initialization\n",
    "def init_spark_session(bucket=''):\n",
    "    '''Trigger SPARK session\n",
    "    Input:\n",
    "    - bucket : S3 bucket name containing images\n",
    "    \n",
    "    Output:\n",
    "    - SparkContext\n",
    "    - S3 bucket images path\n",
    "    '''\n",
    "    \n",
    "    # Remote access to our S3 bucket from Sagemaker\n",
    "    path_img = \"s3a://\"+bucket_name+\"/**\"\n",
    "        \n",
    "#     session = botocore.session.get_session()\n",
    "#     credentials = session.get_credentials()\n",
    "\n",
    "    # TO HIDE CREDENTIALS\n",
    "    access_id = 'AKIAXX4UHDZO6LW34JPX'\n",
    "    access_key = 'DYM7eaUgKPJOlfZZa59kocf4BlaWifM6eYVF6Uth'\n",
    "    \n",
    "    conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))\n",
    "        \n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .config(conf=conf) \\\n",
    "        .config('fs.s3a.access.key', access_id) \\\n",
    "        .config('fs.s3a.secret.key', access_key) \\\n",
    "        .config(\"spark.driver.memory\", \"32g\") \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('P8_Fruits') \\\n",
    "        .getOrCreate()\n",
    "    ) \n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    return sc, spark, path_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b225f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "def load_data(path_img):\n",
    "    '''Dataframes loading: \n",
    "    Input:\n",
    "    - path_image: Directory containing images\n",
    "    \n",
    "    Output:\n",
    "    df_img: Spark dataframe with images and names\n",
    "    '''\n",
    "    # Timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # SPARK dataframe loading\n",
    "    df_img = spark.read.format(\"image\").load(path_img, inferschema=True)\n",
    "    print('Images loaded - DONE')\n",
    "    \n",
    "    df_img = df_img.withColumn(\"fileName\", regexp_replace('image.origin', 'dbfs:/mnt/images/', '')) \n",
    "    split_col =split(df_img['fileName'], '/')\n",
    "    df_img = df_img.withColumn('Category', split_col.getItem(3))\n",
    "    \n",
    "    df_img_see = df_img.select('image', 'image.origin',\"image.height\",\"image.width\",\"image.nChannels\", \"image.mode\", \"image.data\",'Category')\n",
    "    df_img_feat = df_img.select('image.origin',\"image.height\",\"image.width\",\"image.nChannels\", \"image.mode\", \"image.data\",'Category')\n",
    "    \n",
    "    print('Images loaded in: {} secondes'.format(time.strftime('%S', time.gmtime(time.time()-start))))\n",
    "    \n",
    "    return df_img_see, df_img_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88aea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fonction qui donne des informations sur un dataframe spark\n",
    "# def spark_shape(dfs):\n",
    "#     '''Renvoie des informations sur un dataframe spark: \n",
    "#     Entrée:\n",
    "#     - dfs: dataframe spark\n",
    "    \n",
    "#     Retour:\n",
    "#     - nombre enregistrements\n",
    "#       int\n",
    "#     - nombre de colonnes\n",
    "#       int\n",
    "#     '''\n",
    "#     return (dfs.count(), len(dfs.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fonction pour déterminer la catégorie de l'image\n",
    "# def parse_categorie(path):\n",
    "#     '''Renvoie la catégorie d\\'une image à partir de son chemin\n",
    "#     Entrée:\n",
    "#     - chemin complet de l\\'image\n",
    "#       string\n",
    "#     Retour:\n",
    "#     - catégorie de l\\'image\n",
    "#       string\n",
    "#     '''\n",
    "#     if len(path) > 0:\n",
    "#         # Catégorie de l'image\n",
    "#         return path.split('/')[-2]\n",
    "#     else:\n",
    "#         return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "def display_image(dfs, Category):\n",
    "    '''Display a selected image\n",
    "    Input:\n",
    "    - SPARK dataframe\n",
    "    - Image category\n",
    "      \n",
    "    Output:\n",
    "    - Image array\n",
    "      \n",
    "    '''\n",
    "    filter_cat = dfs.filter(dfs.Category == Category)\n",
    "    list_height = filter_cat.select('height').collect()\n",
    "    list_width = filter_cat.select('width').collect()\n",
    "    height = list_height[0].height\n",
    "    width = list_width[0].width\n",
    "\n",
    "    image_1 = filter_cat.first()\n",
    "\n",
    "    disp_img = np.array(image_1.asDict()['image']['data']).reshape(height,width,3)[:,:,::-1]\n",
    "    \n",
    "    return disp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features extraction with VGG16\n",
    "def extract_features_vgg16(bucket_name):\n",
    "    \n",
    "    '''Features extraction with VGG16\n",
    "    Input: S3 bucket name\n",
    "    \n",
    "    Output: Images features\n",
    "    \n",
    "    '''\n",
    "    # Timer\n",
    "    start = time.time()\n",
    "    \n",
    "    model = VGG16(include_top=False, weights='imagenet', pooling='max', input_shape=(224, 224, 3))\n",
    "    model.summary()\n",
    "    \n",
    "    # AWS S3 ressources \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    vgg16_features=[]\n",
    "    \n",
    "    for my_bucket_object in bucket.objects.all():\n",
    "        if my_bucket_object.key.endswith('jpg'):\n",
    "            file_byte_string = s3_client.get_object(Bucket=bucket_name, Key=my_bucket_object.key)['Body'].read()\n",
    "            \n",
    "            # Image loading\n",
    "            img = Image.open(BytesIO(file_byte_string))\n",
    "            \n",
    "            # Image redimensionning in 224*224 px\n",
    "            img_redim = img.resize((224, 224))\n",
    "            \n",
    "            # Image to array\n",
    "            img_array = image.img_to_array(img_redim).reshape((-1,224,224,3))\n",
    "            img_array = np.array(img_array)\n",
    "            \n",
    "            # Images pre-processing \n",
    "            img_array = preprocess_input(img_array)\n",
    "            \n",
    "            # Features extraction for an image\n",
    "            feature = model.predict(img_array).ravel().tolist()\n",
    "            \n",
    "            vgg16_features.append(feature)\n",
    "            \n",
    "    print('Features extraction loading time: {} secondes'.format(time.strftime('%S', time.gmtime(time.time()-start))))\n",
    "    \n",
    "    return vgg16_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab41a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features in SPARK dataframe \n",
    "def features_pyspark_df(features, df_img):\n",
    "    \n",
    "    '''Add features to Pyspark dataframe \n",
    "    Input:\n",
    "    - Image features \n",
    "    \n",
    "    Output:\n",
    "    - pyspark dataframe with info about images and features\n",
    "    '''\n",
    "    features_df = spark.createDataFrame([(l,) for l in features], ['features'])\n",
    "    \n",
    "    df_img = df_img.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "    features_df = features_df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "    df_img_feat = df_img.join(features_df, df_img.row_idx == features_df.row_idx).drop(\"row_idx\")\n",
    "    \n",
    "    return df_img_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be283b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pca(dataframe):\n",
    "    \n",
    "    '''\n",
    "     Data preparation:\n",
    "     - Dense vector conversion\n",
    "     - Standardization\n",
    "     Input : dataframe : Images dataframe\n",
    "     Output : dataframe with standardized dense vectors\n",
    "    '''\n",
    "\n",
    "    # Images data to dense vector conversion\n",
    "    transform_dense_vector = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "    dataframe = dataframe.withColumn('features_vectors', transform_dense_vector('features'))\n",
    "\n",
    "    # Standardization for PCA\n",
    "    scaler_std = StandardScaler(inputCol=\"features_vectors\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "    model_std = scaler_std.fit(dataframe)\n",
    "    # Upscaling\n",
    "    dataframe = model_std.transform(dataframe)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_k_search(dataframe, nb_comp=13):\n",
    "    '''\n",
    "       Search for the optimal k number (95% variance)\n",
    "       param : dataframe : Images dataframe\n",
    "       return : k Number of components explaining 95% of the variance\n",
    "    '''\n",
    "\n",
    "    pca = PCA(k = nb_comp,\n",
    "              inputCol=\"features_scaled\", \n",
    "              outputCol=\"features_pca\")\n",
    "\n",
    "    model_pca = pca.fit(dataframe)\n",
    "    variance = model_pca.explainedVariance\n",
    "\n",
    "    # visuel\n",
    "    plt.plot(np.arange(len(variance)) + 1, variance.cumsum(), c=\"red\", marker='o')\n",
    "    plt.xlabel(\"Nb components\")\n",
    "    plt.ylabel(\"% variance\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "    def nb_comp ():\n",
    "        for i in range(13):\n",
    "          a = variance.cumsum()[i]\n",
    "          if a >= 0.95:\n",
    "              print(\"{} principal components explain 95% of the information\".format(i))\n",
    "              break\n",
    "        return i\n",
    "\n",
    "    k=nb_comp()\n",
    "\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in csv file on a S3 bucket\n",
    "def save_csv_bucket_s3(pca_matrix, file_name, bucket_name):\n",
    "    \n",
    "    '''Saving results in a csv file in a S3 bucket\n",
    "    Input:\n",
    "    - pca_matrix (psypark dataframe)\n",
    "    - csv file name to save\n",
    "    - bucket_name: S3 bucket name\n",
    "    '''\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    \n",
    "    # Buffer creation\n",
    "    csv_buffer = StringIO()\n",
    "    \n",
    "    # psypark to pandas dataframe conversion\n",
    "    pca_matrix.toPandas().to_csv(csv_buffer)\n",
    "    \n",
    "    # Resulting csv file in S3 bucket\n",
    "    s3_resource.Object(bucket_name, file_name).put(Body=csv_buffer.getvalue())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket name\n",
    "bucket_name = 'h7obucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session initialization\n",
    "sc, spark, path = init_spark_session(bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb428d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display pyspark context\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907dc53",
   "metadata": {},
   "source": [
    "### Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe loading\n",
    "images_feat, images_see = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des 5 premières images\n",
    "images_feat['origin', 'Category'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d01f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taille de la dataframe pyspark\n",
    "# spark_shape(images_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8770e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_cat = display_image(images_see, \"apple_rotten_1\")\n",
    "# print(image_cat.shape)\n",
    "# Image.fromarray(image_cat, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68391e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Extraction \n",
    "image_features = extract_features_vgg16(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e020127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding features to pyspark dataframe\n",
    "images_feat_df = features_pyspark_df(image_features, images_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e26c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_feat_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b49965",
   "metadata": {},
   "source": [
    "### PCA Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39730aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = preprocess_pca(images_feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components explaining 95% of the variance\n",
    "n_components = optimal_k_search(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA implementation with optimal k components\n",
    "pca = PCA(k=n_components, inputCol='features_scaled', outputCol='vectors_pca')\n",
    "model_pca = pca.fit(pca_df)\n",
    "\n",
    "# Transform images \n",
    "df_post_pca = model_pca.transform(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_post_pca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results file in new bucket\n",
    "bucket_name_matrix = 'results-fruits-bucket'\n",
    "save_csv_bucket_s3(df_post_pca, 'post_pca_results.csv', bucket_name_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
