{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3c9346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1ec25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de dimension - PCA\n",
    "from pyspark.sql.functions import element_at, split, col, pandas_udf, PandasUDFType, udf\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d3a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context & session\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c01596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow\n",
    "pyarrow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de931f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "# deal with image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6c06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "from pyspark.sql.functions import element_at, col, split\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "# from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from typing import Iterator\n",
    "# ml tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3f0f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ml tasks\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "# transform\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# core featurizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763f8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a Spark session\n",
    "spark = SparkSession.builder.master('local[*]').appName('P8').getOrCreate()\n",
    "# check wether arrow should be enabled by this setting\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ff241d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-EV9IEEEB:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>P8</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2207c31e610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "210bac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define work_path\n",
    "work_path = 'Sample/**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0464d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load with spark.read, elapsed time: 3.20s\n"
     ]
    }
   ],
   "source": [
    "# function to load data into into a spark_df\n",
    "# with spaces of folder's name removed first, inferschema optional here\n",
    "start = time.perf_counter()\n",
    "df_img = spark.read.format('image').load(work_path, inferschema=True) \n",
    "stop = time.perf_counter()\n",
    "print(f'data load with spark.read, elapsed time: {stop - start:0.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97bab32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of images in this sample\n",
    "df_img.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb73ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               image|\n",
      "+--------------------+\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "|{file:///c:/Users...|\n",
      "+--------------------+\n",
      "\n",
      "data load with spark.read, elapsed time: 1.65s\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "df_img.show(20)\n",
    "stop = time.perf_counter()\n",
    "print(f'data load with spark.read, elapsed time: {stop - start:0.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f47df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display DataFrame schema \n",
    "df_img.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ca9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------\n",
      " origin | file:///c:/Users/steph/Documents/Formation_Data_Scientist/P8_Lanchec_Stephane/Sample/cabbage_white_1/r0_172.jpg \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# origin detail\n",
    "df_img.select('image.origin').show(1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163ff746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label from image.origin\n",
    "df_img = df_img.withColumn('label', element_at(split(df_img['image.origin'], \"/\"), -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1358a7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+---------+----+--------------------+---------------+\n",
      "|              origin|height|width|nChannels|mode|                data|          label|\n",
      "+--------------------+------+-----+---------+----+--------------------+---------------+\n",
      "|file:///c:/Users/...|   714|  721|        3|  16|[FF FF FF FF FF F...|cabbage_white_1|\n",
      "|file:///c:/Users/...|   713|  715|        3|  16|[FF FF FF FF FF F...|cabbage_white_1|\n",
      "|file:///c:/Users/...|   711|  713|        3|  16|[FF FF FF FF FF F...|cabbage_white_1|\n",
      "+--------------------+------+-----+---------+----+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show first 3 rows with image struct detailed\n",
    "df_img.select(\n",
    "    'image.origin',\n",
    "    'image.height',\n",
    "    'image.width',\n",
    "    'image.nChannels',\n",
    "    'image.mode',\n",
    "    'image.data',\n",
    "    'label'\n",
    "    ).show(3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c6b12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for featurization, last layers truncated.\n",
    "# nb. spark workers need to access the model and its weights\n",
    "conv_base = VGG16(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    pooling='max',\n",
    "    input_shape=(100, 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01675476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 512)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# verify that the top layer is removed\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d3d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights as broadcasted variable over nodes (provide a copy to each node)\n",
    "conv_base_weights = spark.sparkContext.broadcast(conv_base.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c07e9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make conv_base callable\n",
    "def conv_base_init():\n",
    "    # returns a VGG 16 model with top layer removed and broadcasted weights\n",
    "    conv_base = VGG16(\n",
    "        include_top=False,\n",
    "        weights=None,\n",
    "        pooling='max',\n",
    "        input_shape=(100, 100, 3))\n",
    "    # error if sparkcontext as it will be called on workers and not only drivers\n",
    "    # conv_base_weights = sc.broadcast(conv_base.get_weights())\n",
    "    conv_base.set_weights(conv_base_weights.value)\n",
    "    return conv_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72589836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get tensors from batch path\n",
    "def gettensorfrompath(image_path):\n",
    "    path = image_path.replace(\"file://\", \"\")\n",
    "    img = load_img(path)\n",
    "    x = img_to_array(img)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15e5eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target pandas user defined function to make operation on dataframe with pyspark.sql\n",
    "@pandas_udf('array<double>')\n",
    "def featurize(images_data_iter: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    # load model outside of for loop\n",
    "    conv_base = conv_base_init()\n",
    "    for image_data_series in images_data_iter:\n",
    "        image_path_series = image_data_series['origin']\n",
    "        # Apply functions to entire series at once\n",
    "        x = image_path_series.map(gettensorfrompath)\n",
    "        x = np.stack(list(x.values))\n",
    "        # option is to enable batch_size\n",
    "        features = conv_base.predict(x)\n",
    "        features_flat = [p.flatten() for p in features]\n",
    "        yield pd.Series(features_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2e62998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_img.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e54b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply featurization\n",
    "featurized_df = df_img.withColumn('features', featurize('image')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf611617",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 336, in read_udfs\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 249, in read_single_udf\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 25, in <module>\n    from keras import models\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\models.py\", line 19, in <module>\n    from keras import backend\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 39, in <module>\n    from tensorflow.python.eager.context import get_config\nImportError: cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\steph\\Documents\\Formation_Data_Scientist\\P8_Lanchec_Stephane\\P8_v1.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/steph/Documents/Formation_Data_Scientist/P8_Lanchec_Stephane/P8_v1.ipynb#ch0000025?line=0'>1</a>\u001b[0m featurized_df\u001b[39m.\u001b[39;49mhead(\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py:1589\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=1586'>1587</a>\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(\u001b[39m1\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=1587'>1588</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m rs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=1588'>1589</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(n)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py:728\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=717'>718</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtake\u001b[39m(\u001b[39mself\u001b[39m, num):\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=718'>719</a>\u001b[0m     \u001b[39m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=719'>720</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=720'>721</a>\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=725'>726</a>\u001b[0m \u001b[39m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=726'>727</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=727'>728</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlimit(num)\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py:677\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=666'>667</a>\u001b[0m \u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=667'>668</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=668'>669</a>\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=673'>674</a>\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=674'>675</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=675'>676</a>\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc) \u001b[39mas\u001b[39;00m css:\n\u001b[1;32m--> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=676'>677</a>\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=677'>678</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1297'>1298</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1298'>1299</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1299'>1300</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1300'>1301</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1306'>1307</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 336, in read_udfs\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 249, in read_single_udf\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 25, in <module>\n    from keras import models\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\models.py\", line 19, in <module>\n    from keras import backend\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 39, in <module>\n    from tensorflow.python.eager.context import get_config\nImportError: cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)\n"
     ]
    }
   ],
   "source": [
    "featurized_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0a37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "featurized_df.show()\n",
    "stop = time.perf_counter()\n",
    "print(f'data load with spark.read, elapsed time: {stop - start:0.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of paritions\n",
    "print(featurized_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce469010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transformation(df, n_components=6, col_image='image'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applique un algorithme de PCA sur l'ensemble des images pour réduire la dimension de chaque image \n",
    "    du jeu de données.\n",
    "    \n",
    "    Paramètres:\n",
    "    df(pyspark dataFrame): contient une colonne avec les données images\n",
    "    n_components(int): nombre de dimensions à conserver\n",
    "    col_image(string): nom de la colonne où récupérer les données images\n",
    "    \"\"\"\n",
    "\n",
    "    # Initilisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Les données images sont converties au format vecteur dense\n",
    "    ud_f = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "    df = df.withColumn('image', ud_f('image'))\n",
    "    \n",
    "    standardizer = StandardScaler(inputCol=\"image\", outputCol=\"scaledFeatures\",\n",
    "                                  withStd=True, withMean=True)\n",
    "    model_std = standardizer.fit(df)\n",
    "    df = model_std.transform(df)\n",
    "\n",
    "    # Entrainement de l'algorithme\n",
    "    pca = PCA(k=n_components, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "    model_pca = pca.fit(df)\n",
    "\n",
    "    # Transformation des images sur les k premières composantes\n",
    "    df = model_pca.transform(df)\n",
    "\n",
    "    df = df.filter(df.pcaFeatures.isNotNull())\n",
    "    \n",
    "    # Affiche le temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformation(featurized_df, n_components=4, col_image='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Array to Vectors for PCA\n",
    "array_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "vectorized_df = featurized_df.withColumn('vectors', array_to_vector_udf('features'))\n",
    "vectorized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a08b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce with PCA - set k Max to determine the adequate nb of principal components\n",
    "start = time.perf_counter()\n",
    "pca = PCA(k=20, inputCol='vectors', outputCol='pca_vectors')\n",
    "model = pca.fit(vectorized_df)\n",
    "stop = time.perf_counter()\n",
    "print(f'pca - fit best k nb, elapsed time: {stop - start:0.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1580b",
   "metadata": {},
   "source": [
    "### PCA Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dad26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12447da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scree_plot(pca):\n",
    "    '''Display a scree plot for the pca'''\n",
    "\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(), c=\"red\", marker='o')\n",
    "    plt.xlabel(\"Number of principal components\")\n",
    "    plt.ylabel(\"Percentage explained variance\")\n",
    "    plt.title(\"Scree plot\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da74d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(vectorized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "data = range(1,1000)\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6074bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
