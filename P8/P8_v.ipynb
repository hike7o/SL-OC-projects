{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509ee0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d187a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a842f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries used:\n",
      "Python        : 3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)]\n",
      "pyspark       : 3.1.2\n",
      "PIL           : 9.0.1\n",
      "Numpy         : 1.22.3\n",
      "Pandas        : 1.4.1\n"
     ]
    }
   ],
   "source": [
    "# Versions\n",
    "print('Libraries used:')\n",
    "print('Python        : ' + sys.version)\n",
    "print('pyspark       : ' + pyspark.__version__)\n",
    "print('PIL           : ' + PIL.__version__)\n",
    "print('Numpy         : ' + np.__version__)\n",
    "print('Pandas        : ' + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77d7cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        c:\\Users\\steph\\Documents\\Formation_Data_Scientist\\P8_Lanchec_Stephane\n",
      "PATH_Data:   c:\\Users\\steph\\Documents\\Formation_Data_Scientist\\P8_Lanchec_Stephane/Sample\n",
      "PATH_Result: c:\\Users\\steph\\Documents\\Formation_Data_Scientist\\P8_Lanchec_Stephane/Results\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/Sample'\n",
    "PATH_Result = PATH+'/Results'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9d742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('P8')\n",
    "             .master('local')\n",
    "             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "             .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3567bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f107067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-EV9IEEEB:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>P8</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1bca06d9b80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c2ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1851a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|path                                                                                                         |label          |\n",
      "+-------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|file:/c:/Users/steph/Documents/Formation_Data_Scientist/P8_Lanchec_Stephane/Sample/cabbage_white_1/r0_172.jpg|cabbage_white_1|\n",
      "|file:/c:/Users/steph/Documents/Formation_Data_Scientist/P8_Lanchec_Stephane/Sample/cabbage_white_1/r0_170.jpg|cabbage_white_1|\n",
      "|file:/c:/Users/steph/Documents/Formation_Data_Scientist/P8_Lanchec_Stephane/Sample/cabbage_white_1/r0_168.jpg|cabbage_white_1|\n",
      "|file:/c:/Users/steph/Documents/Formation_Data_Scientist/P8_Lanchec_Stephane/Sample/cucumber_1/r0_198.jpg     |cucumber_1     |\n",
      "|file:/c:/Users/steph/Documents/Formation_Data_Scientist/P8_Lanchec_Stephane/Sample/cucumber_1/r0_196.jpg     |cucumber_1     |\n",
      "+-------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44dc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079fdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fc1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b18f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "514d18a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\pandas\\functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e69235dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = images.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b75eeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steph\\Documents\\Formation_Data_Scientist\\P8_Lanchec_Stephane/Results\n"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5815b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\", line 22, in <module>\n    from . import multiarray\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py\", line 12, in <module>\n    from . import overrides\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\", line 7, in <module>\n    from numpy.core._multiarray_umath import (\nImportError: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 336, in read_udfs\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 249, in read_single_udf\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 21, in <module>\n    from tensorflow.python import tf2\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\n    from tensorflow.python.tools import module_util as _module_util\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\n    from tensorflow.python.eager import context\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 29, in <module>\n    import numpy as np\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\__init__.py\", line 145, in <module>\n    from . import core\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\", line 48, in <module>\n    raise ImportError(msg)\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"C:\\Users\\steph\\anaconda3\\python.exe\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m features_df\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=440'>441</a>\u001b[0m \u001b[39m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=441'>442</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=442'>443</a>\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=480'>481</a>\u001b[0m \u001b[39m name | Bob\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=481'>482</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=482'>483</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=483'>484</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=484'>485</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py?line=485'>486</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mshowString(n, \u001b[39mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1297'>1298</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1298'>1299</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1299'>1300</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1300'>1301</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1306'>1307</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\", line 22, in <module>\n    from . import multiarray\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py\", line 12, in <module>\n    from . import overrides\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\", line 7, in <module>\n    from numpy.core._multiarray_umath import (\nImportError: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 336, in read_udfs\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 249, in read_single_udf\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 69, in read_command\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 21, in <module>\n    from tensorflow.python import tf2\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\n    from tensorflow.python.tools import module_util as _module_util\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\n    from tensorflow.python.eager import context\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 29, in <module>\n    import numpy as np\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\__init__.py\", line 145, in <module>\n    from . import core\n  File \"C:\\Users\\steph\\anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\", line 48, in <module>\n    raise ImportError(msg)\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"C:\\Users\\steph\\anaconda3\\python.exe\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n\n"
     ]
    }
   ],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b35ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84299252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
